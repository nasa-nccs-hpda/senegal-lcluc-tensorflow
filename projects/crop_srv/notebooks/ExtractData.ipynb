{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1b599f0-53a8-453a-bc77-24d766ffbe02",
   "metadata": {},
   "source": [
    "# Extracting data for SRV training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44bdce2a-49cf-4727-8eb9-bddb6f1f7488",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfsm/ccds01/home/appmgr/app/jupyterhub/ilab/kernel/lib/python3.7/site-packages/requests/__init__.py:104: RequestsDependencyWarning: urllib3 (1.26.12) or chardet (5.0.0)/charset_normalizer (2.0.10) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n",
      "2022-10-07 14:11:41.555112: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'unet_regression' from 'tensorflow_caney.networks' (/explore/nobackup/people/jacaraba/development/tensorflow-caney/tensorflow_caney/networks/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1246693/45046458.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/explore/nobackup/people/jacaraba/development/tensorflow-caney'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_caney\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcnn_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_caney\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mseed_everything\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_caney\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/explore/nobackup/people/jacaraba/development/tensorflow-caney/tensorflow_caney/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_caney\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetworks\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0munet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_caney\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetworks\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0munet_regression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_caney\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetworks\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdeeplabv3_plus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpkg_resources\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_distribution\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDistributionNotFound\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'unet_regression' from 'tensorflow_caney.networks' (/explore/nobackup/people/jacaraba/development/tensorflow-caney/tensorflow_caney/networks/__init__.py)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import logging\n",
    "import fiona\n",
    "import rasterio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import rioxarray as rxr\n",
    "import geopandas as gpd\n",
    "import shapely.speedups\n",
    "from IPython.display import display\n",
    "from omegaconf.listconfig import ListConfig\n",
    "from multiprocessing import Pool, cpu_count\n",
    "\n",
    "# import cuspatial\n",
    "import folium\n",
    "import osgeo.gdal\n",
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "from glob import glob\n",
    "from shapely.geometry import box\n",
    "from skimage.transform import resize\n",
    "from sklearn.cluster import KMeans\n",
    "from matplotlib import colors\n",
    "\n",
    "sys.path.append('/explore/nobackup/people/jacaraba/development/tensorflow-caney')\n",
    "\n",
    "from tensorflow_caney.config.cnn_config import Config\n",
    "from tensorflow_caney.utils.system import seed_everything\n",
    "from tensorflow_caney.utils.model import load_model\n",
    "\n",
    "from tensorflow_caney.utils.data import modify_bands, \\\n",
    "    get_mean_std_metadata\n",
    "from tensorflow_caney.utils import indices\n",
    "from tensorflow_caney.inference import inference\n",
    "from tensorflow_caney.utils.data import normalize_image, rescale_image, \\\n",
    "    standardize_batch, standardize_image\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from tensorflow.keras.optimizers import Adam\n",
    "# from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf854d1-bda1-4e37-9848-420199e12362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_filename = '/explore/nobackup/projects/3sl/development/cnn_landcover/crop.exp6/landcover-crop43-0.05.hdf5'\n",
    "model_filename = '/explore/nobackup/projects/3sl/development/cnn_landcover/otcb.all/landcover-otcb59-0.26.hdf5'\n",
    "model = load_model(\n",
    "    model_filename=model_filename,\n",
    "    #model_dir=os.path.join(conf.data_dir, 'model')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea32031-be11-40f2-b00b-372f35840f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.predict(np.random.randint(255, size=(1, 512, 512, 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b834e7-8640-4dac-86e9-2c0f2639c494",
   "metadata": {},
   "outputs": [],
   "source": [
    "srv_data = '/explore/nobackup/projects/3sl/data/VHR/SRV/M1BS/*-toa.tif'\n",
    "srv_label_dir = '/explore/nobackup/projects/3sl/labels/landcover/srv'\n",
    "output_dir = '/explore/nobackup/projects/3sl/development/cnn_landcover/crop.srv.v1/metadata'\n",
    "general_crs = \"EPSG:32628\"\n",
    "start_year = 2009\n",
    "end_year = 2021 # 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c30d991-78dd-4bec-91b7-cfe5c6a66728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create output directory\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5114a567-fbe5-4c3d-ba83-7328526c21e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_worldview_scenes_geometry(data_regex: str, crs: str = None):\n",
    "    \"\"\"\n",
    "    Get WorldView from local EVHR TIFs.\n",
    "    Improvements:\n",
    "        - this could be done way faster with multiprocessing\n",
    "        - optional GPU or CPU versions, add GPU inside function\n",
    "    \"\"\"\n",
    "    # get the paths/filenames of the world view imagery available\n",
    "    filenames = []\n",
    "    if isinstance(data_regex, list):\n",
    "        for regex in data_regex:\n",
    "            filenames.extend(glob(regex))\n",
    "    else:\n",
    "        filenames = glob(data_regex)\n",
    "\n",
    "    # define variables to store the output of the searches\n",
    "    scene_ids_list, bounds_ids_list, years_ids_list, \\\n",
    "        study_area_list = [], [], [], []\n",
    "\n",
    "    if crs is None:\n",
    "        crs = rasterio.open(filenames[0]).crs\n",
    "\n",
    "    for f in filenames:\n",
    "        study_area_list.append(f.split('/')[-3])\n",
    "        scene_ids_list.append(f)\n",
    "        years_ids_list.append(int(Path(f).stem[5:9]))\n",
    "        bounds_ids_list.append(box(*rasterio.open(f).bounds))\n",
    "\n",
    "    d = {\n",
    "        'study_area': study_area_list,\n",
    "        'scene_id': scene_ids_list,\n",
    "        'acq_year': years_ids_list,\n",
    "        'geometry': bounds_ids_list\n",
    "    }\n",
    "    return gpd.GeoDataFrame(d, crs=crs)\n",
    "\n",
    "def filter_gdf_by_list(\n",
    "            gdf,\n",
    "            gdf_key: str = 'acq_year',\n",
    "            isin_list: list = [],\n",
    "            reset_index: bool = True\n",
    "        ):\n",
    "    \"\"\"\n",
    "    Filter GDF by year range.\n",
    "    \"\"\"\n",
    "    return gdf[gdf[gdf_key].isin(isin_list)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae42336-01c5-4e0e-bc6b-7ba9b39a4b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get evhr bounding boxes\n",
    "evhr_gdf = get_worldview_scenes_geometry(srv_data, general_crs)\n",
    "evhr_gdf = evhr_gdf.to_crs(4326)\n",
    "display(evhr_gdf.head())\n",
    "\n",
    "# create folium map\n",
    "total_bounds = evhr_gdf.dissolve().to_crs('+proj=cea').centroid.to_crs(evhr_gdf.crs)\n",
    "m = folium.Map(\n",
    "    location=[total_bounds.y, total_bounds.x],\n",
    "    zoom_start=8,\n",
    "    tiles='https://mt1.google.com/vt/lyrs=s&x={x}&y={y}&z={z}',\n",
    "    attr='Google'\n",
    ")\n",
    "\n",
    "for _, r in evhr_gdf.iterrows():\n",
    "    # Without simplifying the representation of each borough,\n",
    "    # the map might not be displayed\n",
    "    sim_geo = gpd.GeoSeries(r['geometry']).simplify(tolerance=0.001)\n",
    "    geo_j = sim_geo.to_json()\n",
    "    geo_j = folium.GeoJson(data=geo_j,\n",
    "                           style_function=lambda x: {'fillColor': 'orange'})\n",
    "    folium.Popup(str(r['acq_year'])).add_to(geo_j)\n",
    "    geo_j.add_to(m)\n",
    "display(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa99309-85ae-406c-917b-ebaae2edf6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reproject to match worldview projection\n",
    "evhr_gdf = evhr_gdf.to_crs(epsg=general_crs.split(':')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7c726c-6069-4c18-ad99-fb7064a8b477",
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in range(start_year, end_year):\n",
    "\n",
    "    # get label filename\n",
    "    label_filename = glob(os.path.join(srv_label_dir, f'*{year}*.gpkg'))\n",
    "    \n",
    "    # skip iteration if no labels available for this year\n",
    "    if not label_filename:\n",
    "        continue\n",
    "    \n",
    "    label_filename = label_filename[0]\n",
    "    \n",
    "    # iterate over labels\n",
    "    fields_gdf = gpd.read_file(label_filename)\n",
    "    fields_gdf = fields_gdf.to_crs(epsg=general_crs.split(':')[-1])\n",
    "    # print(fields_gdf)\n",
    "    \n",
    "    # iterate over \n",
    "    filtered_evhr_gdf = filter_gdf_by_list(evhr_gdf, 'acq_year', [int(Path(label_filename).stem[-4:])])\n",
    "    # print(filtered_evhr_gdf)\n",
    "\n",
    "    intersection = gpd.sjoin(\n",
    "        fields_gdf, filtered_evhr_gdf[['study_area', 'scene_id', 'acq_year', 'geometry']],\n",
    "        how='left', op='intersects'\n",
    "    )\n",
    "    intersection.dropna(\n",
    "        axis=0,\n",
    "        how='any',\n",
    "        thresh=None,\n",
    "        subset=None,\n",
    "        inplace=True\n",
    "    )\n",
    "    intersection['acq_year'] = intersection['acq_year'].astype(int)\n",
    "    print(f\"{label_filename}, {intersection.shape[0]} matches.\")\n",
    "    \n",
    "    output_filename = os.path.join(output_dir, f'{Path(label_filename).stem}.gpkg')\n",
    "\n",
    "    # save geopackage file within the output_dir\n",
    "    intersection.to_file(output_filename, driver='GPKG', layer='Intersection')\n",
    "\n",
    "#srv_label_filenames = sorted(glob(srv_label))\n",
    "#print(f\"{len(srv_label_filenames)} label filenames.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629ca871-ecee-4060-ad9b-38a504133a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get evhr bounding boxes\n",
    "# evhr_gdf = get_worldview_scenes_geometry(srv_data, general_crs)\n",
    "# evhr_gdf = evhr_gdf.to_crs(4326)\n",
    "intersection = intersection.to_crs(4326)\n",
    "display(intersection.head())\n",
    "\n",
    "# create folium map\n",
    "total_bounds = intersection.dissolve().to_crs('+proj=cea').centroid.to_crs(intersection.crs)\n",
    "m = folium.Map(\n",
    "    location=[total_bounds.y, total_bounds.x],\n",
    "    zoom_start=8,\n",
    "    tiles='https://mt1.google.com/vt/lyrs=s&x={x}&y={y}&z={z}',\n",
    "    attr='Google'\n",
    ")\n",
    "\n",
    "for _, r in intersection.iterrows():\n",
    "    # Without simplifying the representation of each borough,\n",
    "    # the map might not be displayed\n",
    "    sim_geo = gpd.GeoSeries(r['geometry']).simplify(tolerance=0.001)\n",
    "    geo_j = sim_geo.to_json()\n",
    "    geo_j = folium.GeoJson(data=geo_j,\n",
    "                           style_function=lambda x: {'fillColor': 'orange'})\n",
    "    folium.Popup(str(int(r['acq_year']))).add_to(geo_j)\n",
    "    geo_j.add_to(m)\n",
    "display(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd9958c-fa9f-456b-a1b9-81e5e8707cfe",
   "metadata": {},
   "source": [
    "## Second Part: Extracting the actual tiles\n",
    "\n",
    "Once we have a dataframe with the intersection elements - one polygon of the field that is present in the actual worldview image, we proceed to extract the tiles. Here we will take a deeper look at how to extract them in the proper order and location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2424fb6-65d5-4bcb-af67-4611b3c3a34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "regex = '/explore/nobackup/projects/3sl/development/cnn_landcover/crop.srv.v1/metadata/*.gpkg'\n",
    "data_output_dir = '/explore/nobackup/projects/3sl/development/cnn_landcover/crop.srv.v1'\n",
    "tile_size = 256\n",
    "filenames = glob(regex)\n",
    "len(filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d50a320-121c-4adb-8b2e-02911c3679a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import struct, numpy, pylab\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def extractWindow(imageDataset, pixelX, pixelY, pixelWidth, pixelHeight):\n",
    "    # Extract raw data\n",
    "    if type(imageDataset) is np.ndarray:\n",
    "        matrix = imageDataset[int(pixelX):int(pixelX+pixelWidth), int(pixelY):int(pixelY+pixelHeight)]\n",
    "    else:\n",
    "        matrix = imageDataset.ReadAsArray(pixelX, pixelY, pixelWidth, pixelHeight)\n",
    "        print(\"AQUI ESTA EL ERROR\")\n",
    "    #print(pixelX, pixelY, pixelWidth, pixelHeight)\n",
    "    #plt.imshow(matrix[7, :, :] / 10000.0)\n",
    "    #plt.show()\n",
    "    #pylab.show()\n",
    "    # Return\n",
    "    return matrix\n",
    "\n",
    "def extractCenteredWindow(imageDataset, pixelX, pixelY, pixelWidth, pixelHeight):\n",
    "    centeredPixelX = pixelX - pixelWidth / 2\n",
    "    centeredPixelY = pixelY - pixelHeight / 2\n",
    "    return extractWindow(imageDataset, centeredPixelX, centeredPixelY, pixelWidth, pixelHeight)\n",
    "\n",
    "def convertGeoLocationToPixelLocation(geoLocation):\n",
    "    xGeo, yGeo = geoLocation[0], geoLocation[1]\n",
    "    if g2 == 0:\n",
    "        xPixel = (xGeo - g0) / float(g1)\n",
    "        yPixel = (yGeo - g3 - xPixel*g4) / float(g5)\n",
    "    else:\n",
    "        xPixel = (yGeo*g2 - xGeo*g5 + g0*g5 - g2*g3) / float(g2*g4 - g1*g5)\n",
    "        yPixel = (xGeo - g0 - xPixel*g1) / float(g2)\n",
    "    return int(round(xPixel)), int(round(yPixel))\n",
    "\n",
    "def convertGeoDimensionsToPixelDimensions(geoWidth, geoHeight, g1, g5):\n",
    "    return int(round(abs(float(geoWidth) / g1))), int(round(abs(float(geoHeight) / g5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac2fab2-165e-47e8-8be8-e5f478566852",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcad153f-e5aa-40d0-a031-28ce9725a068",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import cv2\n",
    "from skimage.draw import polygon\n",
    "from sklearn.cluster import spectral_clustering\n",
    "from skimage.filters import threshold_otsu, threshold_mean, threshold_isodata, threshold_local\n",
    "from skimage.filters import threshold_otsu, rank\n",
    "from skimage.morphology import disk\n",
    "from  scipy import ndimage\n",
    "from skimage.feature import canny\n",
    "from skimage.filters import sobel\n",
    "from scipy import ndimage as ndi\n",
    "from skimage.segmentation import chan_vese\n",
    "\n",
    "\n",
    "data_output_dir = '/explore/nobackup/projects/3sl/development/cnn_landcover/crop.srv.v1'\n",
    "\n",
    "images_output_dir = os.path.join(data_output_dir, 'images')\n",
    "labels_output_dir = os.path.join(data_output_dir, 'labels')\n",
    "\n",
    "os.makedirs(images_output_dir, exist_ok=True)\n",
    "os.makedirs(labels_output_dir, exist_ok=True)\n",
    "\n",
    "k_means_predictor = None\n",
    "\n",
    "for filename in filenames[4:6]:\n",
    "    \n",
    "    # read gpkg\n",
    "    dataset_gdf = gpd.read_file(filename)\n",
    "    #print(dataset_gdf.shape, dataset_gdf.crs)\n",
    "    display(dataset_gdf.head())\n",
    "    \n",
    "    for index, row in dataset_gdf.iterrows():\n",
    "        \n",
    "        try:\n",
    "            #print(row[\"scene_id\"])\n",
    "            output_data_filename = os.path.join(images_output_dir, f'{Path(row[\"scene_id\"]).stem}_{str(index+1)}.npy')\n",
    "            output_label_filename = os.path.join(labels_output_dir, f'{Path(row[\"scene_id\"]).stem}_{str(index+1)}.npy')\n",
    "\n",
    "            #------------------------------------------------------------------------------\n",
    "            # here we extract the data tile\n",
    "            #------------------------------------------------------------------------------\n",
    "            imageDataset = osgeo.gdal.Open(row['scene_id'])\n",
    "\n",
    "            polygon_centroid = row['geometry'].centroid\n",
    "            print(\"centroid\", polygon_centroid.x, polygon_centroid.y, polygon_centroid)\n",
    "\n",
    "            g0, g1, g2, g3, g4, g5 = imageDataset.GetGeoTransform()\n",
    "            print(\n",
    "                f\"g0={g0}, g1={g1}, g2={g2}, g3={g3}, g4={g4}, g5={g5}, {imageDataset.RasterXSize}, {imageDataset.RasterYSize}\")\n",
    "\n",
    "            windowPixelX, windowPixelY = convertGeoLocationToPixelLocation(\n",
    "                [row['geometry'].centroid.x, row['geometry'].centroid.y])\n",
    "            print(\"Pixel location: \", windowPixelX, windowPixelY)\n",
    "\n",
    "            windowPixelWidth, windowPixelHeight = tile_size, tile_size\n",
    "            data_matrix = extractCenteredWindow(\n",
    "                imageDataset, windowPixelX, windowPixelY, windowPixelWidth, windowPixelHeight)\n",
    "            data_matrix = np.moveaxis(data_matrix, 0, -1)\n",
    "\n",
    "            # if nodata is present, skip\n",
    "            if data_matrix.min() < 0:\n",
    "                continue\n",
    "                \n",
    "            #------------------------------------------------------------------------------\n",
    "            # here we extract the label tile\n",
    "            #------------------------------------------------------------------------------\n",
    "            label_mask = numpy.full((imageDataset.RasterXSize,imageDataset.RasterYSize), False)\n",
    "            polygon_pixel_coords = np.apply_along_axis(\n",
    "                convertGeoLocationToPixelLocation, axis=1, arr=np.array(list(row['geometry'].exterior.coords)))\n",
    "            rr, cc = polygon(\n",
    "                polygon_pixel_coords[:,0], polygon_pixel_coords[:,1],\n",
    "                (imageDataset.RasterXSize,imageDataset.RasterYSize)\n",
    "            )\n",
    "            label_mask[cc, rr] = True\n",
    "\n",
    "            # there is a strange represenation of where X and Y are location within the polygon\n",
    "            # for now just invert the axis and call it a day\n",
    "            label_matrix = extractCenteredWindow(\n",
    "                label_mask.astype(int), windowPixelY, windowPixelX, windowPixelWidth, windowPixelHeight)\n",
    "\n",
    "            #print(\"occurrence\", np.count_nonzero(label_matrix == 1), np.unique(label_matrix))\n",
    "\n",
    "            if np.count_nonzero(label_matrix == 1) < 10000: #20000:  # 512x512 / 2\n",
    "                continue\n",
    "            \n",
    "            # assert if a tile is smaller than the given size we want\n",
    "            if label_matrix.shape[0] != windowPixelWidth or label_matrix.shape[1] != windowPixelHeight:\n",
    "                continue\n",
    "\n",
    "            np.save(output_data_filename, data_matrix)\n",
    "            np.save(output_label_filename, label_matrix)\n",
    "            \n",
    "            # grab the number of bands in the image, naip images have four bands\n",
    "            nbands = data_matrix.shape[-1]\n",
    "            band_s = 6\n",
    "\n",
    "            # create an empty array in which each column will hold a flattened band\n",
    "            flat_data = np.empty((data_matrix.shape[0]*data_matrix.shape[1], nbands))\n",
    "\n",
    "            # loop through each band in the image and add to the data array\n",
    "            data_matrix_2 = standardize_image(data_matrix/10000.0, 'local')\n",
    "            \n",
    "            for i in range(nbands):\n",
    "                band = data_matrix_2[:, :, i]\n",
    "                flat_data[:, i-1] = band.flatten()\n",
    "                        \n",
    "            #if k_means_predictor is None:\n",
    "            k_means_predictor = KMeans(n_clusters=2, n_init=1, random_state=24, algorithm=\"elkan\").fit(flat_data)\n",
    "            \n",
    "            #print(k_means_predictor.labels_)\n",
    "            kmeans_predicted = k_means_predictor.predict(flat_data).reshape((data_matrix.shape[0], data_matrix.shape[1]))\n",
    "\n",
    "            #predicted = spectral_clustering(data_matrix[:, :, 6], n_clusters=2, eigen_solver=\"arpack\")\n",
    "\n",
    "            #plt.figure(figsize = (6,6))\n",
    "            #plt.imshow(data_matrix[:, :, 6] / 10000.0)\n",
    "            #plt.imshow(label_matrix, alpha=0.5)\n",
    "            #plt.imshow(predicted, alpha=0.2, cmap=colors.ListedColormap(['k','r']))\n",
    "            #plt.show()\n",
    "            \n",
    "            #img = cv2.cvtColor(data_matrix[:, :, 4],cv2.COLOR_BGR2RGB)\n",
    "            thresh_otsu = threshold_otsu(data_matrix[:, :, band_s])\n",
    "            thresh_mean = threshold_mean(data_matrix[:, :, band_s])\n",
    "            thresh_isodata = threshold_isodata(data_matrix[:, :, band_s])\n",
    "            thresh_local = threshold_local(data_matrix[:, :, band_s], block_size=201, offset=10)\n",
    "            local_otsu = rank.otsu(data_matrix[:, :, band_s], disk(15))\n",
    "            #predicted = data_matrix[:, :, 6] > thresh\n",
    "            #print(binary.shape)\n",
    "            \n",
    "            #final_label = label_matrix + (data_matrix[:, :, band_s] > thresh_mean)\n",
    "            #final_label[final_label > 1] = 1\n",
    "            # PREDICT TILE\n",
    "            \n",
    "            #pred_image = standardize_image(data_matrix/10000.0, 'local')\n",
    "            #pred_image = model.predict(np.expand_dims(pred_image, axis=0))\n",
    "            #pred_image = np.squeeze(np.argmax(pred_image, axis=-1))\n",
    "            #print(pred_image.shape, np.unique(pred_image))\n",
    "            \n",
    "            #pred_image[pred_image == 1] = 0\n",
    "            #pred_image[pred_image == 2] = 1\n",
    "            #pred_image[pred_image == 3] = 0\n",
    "            #pred_image[pred_image == 4] = 0\n",
    "\n",
    "            #final_label = pred_image\n",
    "            #ndvi = (data_matrix[:, :, 6] - data_matrix[:, :, 5]) / (data_matrix[:, :, 6] + data_matrix[:, :, 5])\n",
    "            #print(ndvi.shape, ndvi.min(), ndvi.max())\n",
    "            #ndvi = canny(data_matrix[:, :, 6]/10000.0)\n",
    "            #ndvi = sobel(data_matrix[:, :, 6]/10000.0)\n",
    "            #ndvi = ndi.binary_fill_holes(ndvi)\n",
    "            print(\"sobel\", ndvi.min(), ndvi.max())\n",
    "            ndvi = chan_vese(data_matrix[:, :, 6]/10000.0)\n",
    "            #ndvi = ndvi\n",
    "            # final_label = ndimage.binary_fill_holes((data_matrix[:, :, band_s] > thresh_mean)).astype(int)\n",
    "\n",
    "            \n",
    "            fig, (ax1, ax2, ax3, ax4, ax5, ax6, ax7) = plt.subplots(1, 7, figsize=(20, 10) )\n",
    "            ax1.imshow(data_matrix[:, :, band_s] / 10000.0)\n",
    "            ax1.imshow(label_matrix, alpha=0.5)\n",
    "            ax2.imshow(label_matrix, cmap=colors.ListedColormap(['k','r']))\n",
    "            ax3.imshow(data_matrix[:, :, band_s] > thresh_otsu, cmap=colors.ListedColormap(['k','r']))\n",
    "            ax4.imshow(data_matrix[:, :, band_s] > thresh_mean, cmap=colors.ListedColormap(['k','r']))\n",
    "            ax5.imshow(data_matrix[:, :, band_s] > thresh_isodata, cmap=colors.ListedColormap(['k','r']))\n",
    "            ax6.imshow(kmeans_predicted, cmap=colors.ListedColormap(['k','r']))\n",
    "            #ax7.imshow(final_label, cmap=colors.ListedColormap(['k','r']))\n",
    "            ax7.imshow(ndvi)\n",
    "            \n",
    "            ax1.set_title('NIR1 + Alpha Original Label')\n",
    "            ax2.set_title('Label Matrix')\n",
    "            ax3.set_title('Otsu Threshold')\n",
    "            ax4.set_title('Mean Threshold')\n",
    "            ax5.set_title('Isodata Threshold')\n",
    "            ax6.set_title('K-Means')\n",
    "            ax7.set_title('Final Label Tile (Mean Shift + Label)')\n",
    "\n",
    "            plt.show()\n",
    "            \n",
    "            #print(data_matrix.shape, data_matrix[:, :, 6])\n",
    "            #labels = spectral_clustering(data_matrix[:, :, 6], n_clusters=2, eigen_solver=\"arpack\")\n",
    "            #print(labels, labels.shape)\n",
    "            \n",
    "            # from sklearn.cluster import MeanShift\n",
    "            \n",
    "        except (AttributeError, IndexError) as e:\n",
    "            print(e)\n",
    "            continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d3fc7b-14a2-4fcc-b0b1-44b5f22f015c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98220067-6907-4cbe-9768-27362893d96a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d706bd-7e2d-4dfa-a71e-5b3a34a09e07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ilab-kernel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
